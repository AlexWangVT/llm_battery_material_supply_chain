{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pymongo import MongoClient\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, pipeline\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from langchain_community.document_loaders.mongodb import MongodbLoader\n",
    "from langchain.vectorstores import Weaviate\n",
    "from bs4 import BeautifulSoup  \n",
    "from langchain_community.document_loaders import UnstructuredURLLoader, UnstructuredPDFLoader, PyPDFLoader\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, unquote\n",
    "import weaviate\n",
    "from weaviate.connect import ConnectionParams\n",
    "from weaviate.collections.classes.config import CollectionConfig, Property, DataType, VectorizerConfig\n",
    "import asyncio\n",
    "from langchain_community.tools import DuckDuckGoSearchRun, DuckDuckGoSearchResults\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA \n",
    "import torch\n",
    "import accelerate\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwang/workspace/llm_battery_material_supply_chain/.venv3.12/lib/python3.12/site-packages/torch/cuda/__init__.py:789: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 70.76it/s]\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use defined Langchain agent to search documents automatically\n",
    "# The agent is combined search tool with a LLM\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # Or another quantized variant\n",
    "token = os.getenv(\"LLaMA_API_KEY\") # huggingface token\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\",  # Automatically use GPU if available\n",
    "                                                torch_dtype=torch.float16, token=token)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256) \n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage Links:\n",
      "- https://money.usnews.com/investing/news/articles/2025-02-13/exclusive-chinas-byd-holds-mining-rights-in-brazils-lithium-valley-documents-show\n",
      "- https://procurementmag.com/supply-chain-management/byd-expands-into-brazils-lithium-valley-with-acquisition\n",
      "- https://engineerine.com/byd-blade-battery/\n",
      "- https://interestingengineering.com/transportation/china-byd-secures-lithium-mining\n",
      "\n",
      "Summary:\n",
      "* **BYD secures lithium mining rights in Brazil:**  BYD, a major Chinese electric vehicle (EV) manufacturer, has acquired mining rights covering a significant area in Brazil's lithium-rich Jequitinhonha Valley, strategically positioning itself near its planned EV factory in Bahia.\n",
      "\n",
      "* **Strategic location for EV production:** This acquisition directly supports BYD's new EV factory in Bahia, reducing transportation costs and securing a crucial raw material supply for its EV production.\n",
      "\n",
      "* **Conflicting reports on BYD's battery technology:** While BYD is investing in lithium mining, other reports highlight its development of lithium-ion-free sodium-ion batteries, suggesting a diversification strategy to reduce reliance on lithium and potentially lower costs and environmental impact.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use structured search tool\n",
    "search_tool = DuckDuckGoSearchResults(output_format=\"list\")\n",
    "# query = \"Find 3 useful documents (including web articles, PDFs, and others) about battery materials in supply chains, at least of them should be from a paper in PDF\"\n",
    "query = \"find the site locations of BYD's battery material lithium, return as (latitude, longitude)\"\n",
    "search_results = search_tool.invoke(query)\n",
    "\n",
    "# Print webpage links\n",
    "print(\"Webpage Links:\")\n",
    "for result in search_results[:100]: \n",
    "    print(\"-\", result[\"link\"])\n",
    "\n",
    "# Combine snippets for summarization\n",
    "combined_snippets = \"\\n\".join(f\"{r['title']}: {r['snippet']}\" for r in search_results[:3])\n",
    "\n",
    "# LLM prompt\n",
    "summary_prompt = f\"\"\"You are a helpful assistant.\n",
    "\n",
    "Here are some search result summaries:\n",
    "{combined_snippets}\n",
    "\n",
    "Please summarize the most useful insights in 3 bullet points.\n",
    "\"\"\"\n",
    "\n",
    "response = llm(summary_prompt)\n",
    "print(\"\\nSummary:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nHow to load PDFs | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to load PDFsOn this pageHow to load PDFs\\nPortable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.\\nThis guide covers how to load PDF documents into the LangChain Document format that we use downstream.\\nText in PDFs is typically represented via text boxes. They may also contain images. A PDF parser might do some combination of the following:\\n\\nAgglomerate text boxes into lines, paragraphs, and other structures via heuristics or ML inference;\\nRun OCR on images to detect text therein;\\nClassify text as belonging to paragraphs, lists, tables, or other structures;\\nStructure text into table rows and columns, or key-value pairs.\\n\\nLangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your needs. Below we enumerate the possibilities.\\nWe will demonstrate these approaches on a sample file:\\nfile_path = (    \"../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf\")\\nA note on multimodal modelsMany modern LLMs support inference over multimodal inputs (e.g., images). In some applications -- such as question-answering over PDFs with complex layouts, diagrams, or scans -- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. We demonstrate an example of this in the Use of multimodal models section below.\\nSimple and fast text extraction‚Äã\\nIf you are looking for a simple string representation of text that is embedded in a PDF, the method below is appropriate. It will return a list of Document objects-- one per page-- containing a single string of the page\\'s text in the Document\\'s page_content attribute. It will not parse text in images or scanned PDF pages. Under the hood it uses the pypdf Python library.\\nLangChain document loaders implement lazy_load and its async variant, alazy_load, which return iterators of Document objects. We will use these below.\\n%pip install -qU pypdf\\nfrom langchain_community.document_loaders import PyPDFLoaderloader = PyPDFLoader(file_path)pages = []async for page in loader.alazy_load():    pages.append(page)API Reference:PyPDFLoader\\nprint(f\"{pages[0].metadata}\\\\n\")print(pages[0].page_content)\\n{\\'source\\': \\'../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf\\', \\'page\\': 0}LayoutParser : A UniÔ¨Åed Toolkit for DeepLearning Based Document Image AnalysisZejiang Shen1( ÔøΩ), Ruochen Zhang2, Melissa Dell3, Benjamin Charles GermainLee4, Jacob Carlson3, and Weining Li51Allen Institute for AIshannons@allenai.org2Brown Universityruochen zhang@brown.edu3Harvard University{melissadell,jacob carlson }@fas.harvard.edu4University of Washingtonbcgl@cs.washington.edu5University of Waterloow422li@uwaterloo.caAbstract. Recent advances in document image analysis (DIA) have beenprimarily driven by the application of neural networks. Ideally, researchoutcomes could be easily deployed in production and extended for furtherinvestigation. However, various factors like loosely organized codebasesand sophisticated model conÔ¨Ågurations complicate the easy reuse of im-portant innovations by a wide audience. Though there have been on-goingeÔ¨Äorts to improve reusability and simplify deep learning (DL) modeldevelopment in disciplines like natural language processing and computervision, none of them are optimized for challenges in the domain of DIA.This represents a major gap in the existing toolkit, as DIA is central toacademic research across a wide range of disciplines in the social sciencesand humanities. This paper introduces LayoutParser , an open-sourcelibrary for streamlining the usage of DL in DIA research and applica-tions. The core LayoutParser library comes with a set of simple andintuitive interfaces for applying and customizing DL models for layout de-tection, character recognition, and many other document processing tasks.To promote extensibility, LayoutParser also incorporates a communityplatform for sharing both pre-trained models and full document digiti-zation pipelines. We demonstrate that LayoutParser is helpful for bothlightweight and large-scale digitization pipelines in real-word use cases.The library is publicly available at https://layout-parser.github.io .Keywords: Document Image Analysis ¬∑Deep Learning ¬∑Layout Analysis¬∑Character Recognition ¬∑Open Source library ¬∑Toolkit.1 IntroductionDeep Learning(DL)-based approaches are the state-of-the-art for a wide range ofdocument image analysis (DIA) tasks including document image classiÔ¨Åcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\\nNote that the metadata of each document stores the corresponding page number.\\nVector search over PDFs‚Äã\\nOnce we have loaded PDFs into LangChain Document objects, we can index them (e.g., a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain embeddings model will suffice.\\n%pip install -qU langchain-openai\\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\nfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsvector_store = InMemoryVectorStore.from_documents(pages, OpenAIEmbeddings())docs = vector_store.similarity_search(\"What is LayoutParser?\", k=2)for doc in docs:    print(f\\'Page {doc.metadata[\"page\"]}: {doc.page_content[:300]}\\\\n\\')API Reference:InMemoryVectorStore | OpenAIEmbeddings\\nPage 13: 14 Z. Shen et al.6 ConclusionLayoutParser provides a comprehensive toolkit for deep learning-based documentimage analysis. The oÔ¨Ä-the-shelf library is easy to install, and can be used tobuild Ô¨Çexible and accurate pipelines for processing documents with complicatedstructures. It also supports hiPage 0: LayoutParser : A UniÔ¨Åed Toolkit for DeepLearning Based Document Image AnalysisZejiang Shen1( ÔøΩ), Ruochen Zhang2, Melissa Dell3, Benjamin Charles GermainLee4, Jacob Carlson3, and Weining Li51Allen Institute for AIshannons@allenai.org2Brown Universityruochen zhang@brown.edu3Harvard University\\nLayout analysis and extraction of text from images‚Äã\\nIf you require a more granular segmentation of text (e.g., into distinct paragraphs, titles, tables, or other structures) or require extraction of text from images, the method below is appropriate. It will return a list of Document objects, where each object represents a structure on the page. The Document\\'s metadata stores the page number and other information related to the object (e.g., it might store table rows and columns in the case of a table object).\\nUnder the hood it uses the langchain-unstructured library. See the integration docs for more information about using Unstructured with LangChain.\\nUnstructured supports multiple parameters for PDF parsing:\\n\\nstrategy (e.g., \"fast\" or \"hi-res\")\\nAPI or local processing. You will need an API key to use the API.\\n\\nThe hi-res strategy provides support for document layout analysis and OCR. We demonstrate it below via the API. See local parsing section below for considerations when running locally.\\n%pip install -qU langchain-unstructured\\nimport getpassimport osif \"UNSTRUCTURED_API_KEY\" not in os.environ:    os.environ[\"UNSTRUCTURED_API_KEY\"] = getpass.getpass(\"Unstructured API Key:\")\\nUnstructured API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\\nAs before, we initialize a loader and load documents lazily:\\nfrom langchain_unstructured import UnstructuredLoaderloader = UnstructuredLoader(    file_path=file_path,    strategy=\"hi_res\",    partition_via_api=True,    coordinates=True,)docs = []for doc in loader.lazy_load():    docs.append(doc)API Reference:UnstructuredLoader\\nINFO: Preparing to split document for partition.INFO: Starting page number set to 1INFO: Allow failed set to 0INFO: Concurrency level set to 5INFO: Splitting pages 1 to 16 (16 total)INFO: Determined optimal split size of 4 pages.INFO: Partitioning 4 files with 4 page(s) each.INFO: Partitioning set #1 (pages 1-4).INFO: Partitioning set #2 (pages 5-8).INFO: Partitioning set #3 (pages 9-12).INFO: Partitioning set #4 (pages 13-16).INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: Successfully partitioned set #1, elements added to the final result.INFO: Successfully partitioned set #2, elements added to the final result.INFO: Successfully partitioned set #3, elements added to the final result.INFO: Successfully partitioned set #4, elements added to the final result.\\nHere we recover 171 distinct structures over the 16 page document:\\nprint(len(docs))\\n171\\nWe can use the document metadata to recover content from a single page:\\nfirst_page_docs = [doc for doc in docs if doc.metadata.get(\"page_number\") == 1]for doc in first_page_docs:    print(doc.page_content)\\nLayoutParser: A UniÔ¨Åed Toolkit for Deep Learning Based Document Image Analysis1 2 0 2 n u J 1 2 ] V C . s c [ 2 v 8 4 3 5 1 . 3 0 1 2 : v i X r aZejiang Shen¬Æ (<), Ruochen Zhang?, Melissa Dell¬Æ, Benjamin Charles Germain Lee?, Jacob Carlson¬Æ, and Weining Li¬Æ1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.caAbstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conÔ¨Ågurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eÔ¨Äorts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applica- tions. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout de- tection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digiti- zation pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.Keywords: Document Image Analysis ¬∑ Deep Learning ¬∑ Layout Analysis ¬∑ Character Recognition ¬∑ Open Source library ¬∑ Toolkit.1 IntroductionDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classiÔ¨Åcation [11,\\nExtracting tables and other structures‚Äã\\nEach Document we load represents a structure, like a title, paragraph, or table.\\nSome structures may be of special interest for indexing or question-answering tasks. These structures may be:\\n\\nClassified for easy identification;\\nParsed into a more structured representation.\\n\\nBelow, we identify and extract a table:\\nClick to expand code for rendering pages%pip install -qU matplotlib PyMuPDF pillowimport fitzimport matplotlib.patches as patchesimport matplotlib.pyplot as pltfrom PIL import Imagedef plot_pdf_with_boxes(pdf_page, segments):    pix = pdf_page.get_pixmap()    pil_image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)    fig, ax = plt.subplots(1, figsize=(10, 10))    ax.imshow(pil_image)    categories = set()    category_to_color = {        \"Title\": \"orchid\",        \"Image\": \"forestgreen\",        \"Table\": \"tomato\",    }    for segment in segments:        points = segment[\"coordinates\"][\"points\"]        layout_width = segment[\"coordinates\"][\"layout_width\"]        layout_height = segment[\"coordinates\"][\"layout_height\"]        scaled_points = [            (x * pix.width / layout_width, y * pix.height / layout_height)            for x, y in points        ]        box_color = category_to_color.get(segment[\"category\"], \"deepskyblue\")        categories.add(segment[\"category\"])        rect = patches.Polygon(            scaled_points, linewidth=1, edgecolor=box_color, facecolor=\"none\"        )        ax.add_patch(rect)    # Make legend    legend_handles = [patches.Patch(color=\"deepskyblue\", label=\"Text\")]    for category in [\"Title\", \"Image\", \"Table\"]:        if category in categories:            legend_handles.append(                patches.Patch(color=category_to_color[category], label=category)            )    ax.axis(\"off\")    ax.legend(handles=legend_handles, loc=\"upper right\")    plt.tight_layout()    plt.show()def render_page(doc_list: list, page_number: int, print_text=True) -> None:    pdf_page = fitz.open(file_path).load_page(page_number - 1)    page_docs = [        doc for doc in doc_list if doc.metadata.get(\"page_number\") == page_number    ]    segments = [doc.metadata for doc in page_docs]    plot_pdf_with_boxes(pdf_page, segments)    if print_text:        for doc in page_docs:            print(f\"{doc.page_content}\\\\n\")\\nrender_page(docs, 5)\\n\\nLayoutParser: A UniÔ¨Åed Toolkit for DL-Based DIA5Table 1: Current layout detection models in the LayoutParser model zooDataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiÔ¨Åc documents Layouts of scanned modern magazines and scientiÔ¨Åc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiÔ¨Åc and business document Layouts of history Japanese documents1 For each dataset, we train several models of diÔ¨Äerent sizes for diÔ¨Äerent needs (the trade-oÔ¨Ä between accuracy vs. computational cost). For ‚Äúbase model‚Äù and ‚Äúlarge model‚Äù, we refer to using the ResNet 50 or ResNet 101 backbones [13], respectively. One can train models of diÔ¨Äerent architectures, like Faster R-CNN [28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model zoo in coming months.layout data structures, which are optimized for eÔ¨Éciency and versatility. 3) When necessary, users can employ existing or customized OCR models via the uniÔ¨Åed API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component.3.1 Layout Detection ModelsIn LayoutParser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. DiÔ¨Äerent from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CNN [28] and Mask R-CNN [12] are used. This yields prediction results of high accuracy and makes it possible to build a concise, generalized interface for layout detection. LayoutParser, built upon Detectron2 [35], provides a minimal API that can perform layout detection with only four lines of code in Python:1 import layoutparser as lp 2 image = cv2 . imread ( \" image_file \" ) # load images 3 model = lp . De t e c tro n2 Lay outM odel ( \" lp :// PubLayNet / f as t er _ r c nn _ R _ 50 _ F P N_ 3 x / config \" ) 4 5 layout = model . detect ( image )LayoutParser provides a wealth of pre-trained model weights using various datasets covering diÔ¨Äerent languages, time periods, and document types. Due to domain shift [7], the prediction performance can notably drop when models are ap- plied to target samples that are signiÔ¨Åcantly diÔ¨Äerent from the training dataset. As document structures and layouts vary greatly in diÔ¨Äerent domains, it is important to select models trained on a dataset similar to the test samples. A semantic syntax is used for initializing the model weights in LayoutParser, using both the dataset name and model name lp://<dataset-name>/<model-architecture-name>.\\nNote that although the table text is collapsed into a single string in the document\\'s content, the metadata contains a representation of its rows and columns:\\nfrom IPython.display import HTML, displaysegments = [    doc.metadata    for doc in docs    if doc.metadata.get(\"page_number\") == 5 and doc.metadata.get(\"category\") == \"Table\"]display(HTML(segments[0][\"text_as_html\"]))\\nable 1. LUllclll 1ayoul actCCLloll 1110AdCs 111 L1C LayoOulralsel 1110U4cl 200Dataset| Base Model\\'|NotesPubLayNet [38]F/MLayouts of modern scientific documentsPRImAMLayouts of scanned modern magazines and scientific reportsNewspaperFLayouts of scanned US newspapers from the 20th centuryTableBank [18]FTable region on modern scientific and business documentHJDatasetF/MLayouts of history Japanese documents\\nExtracting text from specific sections‚Äã\\nStructures may have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding Document objects.\\nBelow, we extract all text associated with the document\\'s \"Conclusion\" section:\\nrender_page(docs, 14, print_text=False)\\n\\nconclusion_docs = []parent_id = -1for doc in docs:    if doc.metadata[\"category\"] == \"Title\" and \"Conclusion\" in doc.page_content:        parent_id = doc.metadata[\"element_id\"]    if doc.metadata.get(\"parent_id\") == parent_id:        conclusion_docs.append(doc)for doc in conclusion_docs:    print(doc.page_content)\\nLayoutParser provides a comprehensive toolkit for deep learning-based document image analysis. The oÔ¨Ä-the-shelf library is easy to install, and can be used to build Ô¨Çexible and accurate pipelines for processing documents with complicated structures. It also supports high-level customization and enables easy labeling and training of DL models on unique document image datasets. The LayoutParser community platform facilitates sharing DL models and DIA pipelines, inviting discussion and promoting code reproducibility and reusability. The LayoutParser team is committed to keeping the library updated continuously and bringing the most recent advances in DL-based DIA, such as multi-modal document modeling [37, 36, 9] (an upcoming priority), to a diverse audience of end-users.Acknowledgements We thank the anonymous reviewers for their comments and suggestions. This project is supported in part by NSF Grant OIA-2033558 and funding from the Harvard Data Science Initiative and Harvard Catalyst. Zejiang Shen thanks Doug Downey for suggestions.\\nExtracting text from images‚Äã\\nOCR is run on images, enabling the extraction of text therein:\\nrender_page(docs, 11)\\n\\nLayoutParser: A UniÔ¨Åed Toolkit for DL-Based DIAfocuses on precision, eÔ¨Éciency, and robustness. The target documents may have complicated structures, and may require training multiple layout detection models to achieve the optimal accuracy. Light-weight pipelines are built for relatively simple documents, with an emphasis on development ease, speed and Ô¨Çexibility. Ideally one only needs to use existing resources, and model training should be avoided. Through two exemplar projects, we show how practitioners in both academia and industry can easily build such pipelines using LayoutParser and extract high-quality structured document data for their downstream tasks. The source code for these projects will be publicly available in the LayoutParser community hub.115.1 A Comprehensive Historical Document Digitization PipelineThe digitization of historical documents can unlock valuable data that can shed light on many important social, economic, and historical questions. Yet due to scan noises, page wearing, and the prevalence of complicated layout structures, ob- taining a structured representation of historical document scans is often extremely complicated. In this example, LayoutParser was used to develop a comprehensive pipeline, shown in Figure 5, to gener- ate high-quality structured data from historical Japanese Ô¨Årm Ô¨Ånancial ta- bles with complicated layouts. The pipeline applies two layout models to identify diÔ¨Äerent levels of document structures and two customized OCR engines for optimized character recog- nition accuracy.‚ÄòActive Learning Layout Annotate Layout Dataset | +‚Äî‚Äî Annotation Toolkit A4 Deep Learning Layout Layout Detection Model Training & Inference, A Post-processing ‚Äî Handy Data Structures & \\\\ Lo orajport 7 ) Al Pls for Layout Data A4 Default and Customized Text Recognition 0CR Models ¬• Visualization & Export Layout Structure Visualization & Storage The Japanese Document Helpful LayoutParser Modules Digitization PipelineAs shown in Figure 4 (a), the document contains columns of text written vertically 15, a common style in Japanese. Due to scanning noise and archaic printing technology, the columns can be skewed or have vari- able widths, and hence cannot be eas- ily identiÔ¨Åed via rule-based methods. Within each column, words are sepa- rated by white spaces of variable size, and the vertical positions of objects can be an indicator of their layout type.Fig. 5: Illustration of how LayoutParser helps with the historical document digi- tization pipeline.15 A document page consists of eight rows like this. For simplicity we skip the row segmentation discussion and refer readers to the source code when available.\\nNote that the text from the figure on the right is extracted and incorporated into the content of the Document.\\nLocal parsing‚Äã\\nParsing locally requires the installation of additional dependencies.\\nPoppler (PDF analysis)\\n\\nLinux: apt-get install poppler-utils\\nMac: brew install poppler\\nWindows: https://github.com/oschwartz10612/poppler-windows\\n\\nTesseract (OCR)\\n\\nLinux: apt-get install tesseract-ocr\\nMac: brew install tesseract\\nWindows: https://github.com/UB-Mannheim/tesseract/wiki#tesseract-installer-for-windows\\n\\nWe will also need to install the unstructured PDF extras:\\n%pip install -qU \"unstructured[pdf]\"\\nWe can then use the UnstructuredLoader much the same way, forgoing the API key and partition_via_api setting:\\nloader_local = UnstructuredLoader(    file_path=file_path,    strategy=\"hi_res\",)docs_local = []for doc in loader_local.lazy_load():    docs_local.append(doc)\\nWARNING: This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model nameINFO: Reading PDF for file: /Users/chestercurme/repos/langchain/libs/community/tests/integration_tests/examples/layout-parser-paper.pdf ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: padding image by 20 for structure detectionINFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: padding image by 20 for structure detectionINFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...\\nThe list of documents can then be processed similarly to those obtained from the API.\\nUse of multimodal models‚Äã\\nMany modern LLMs support inference over multimodal inputs (e.g., images). In some applications-- such as question-answering over PDFs with complex layouts, diagrams, or scans-- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. This allows a model to reason over the two dimensional content on the page, instead of a \"one-dimensional\" string representation.\\nIn principle we can use any LangChain chat model that supports multimodal inputs. A list of these models is documented here. Below we use OpenAI\\'s gpt-4o-mini.\\nFirst we define a short utility function to convert a PDF page to a base64-encoded image:\\n%pip install -qU PyMuPDF pillow langchain-openai\\nimport base64import ioimport fitzfrom PIL import Imagedef pdf_page_to_base64(pdf_path: str, page_number: int):    pdf_document = fitz.open(pdf_path)    page = pdf_document.load_page(page_number - 1)  # input is one-indexed    pix = page.get_pixmap()    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)    buffer = io.BytesIO()    img.save(buffer, format=\"PNG\")    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\\nfrom IPython.display import Image as IPImagefrom IPython.display import displaybase64_image = pdf_page_to_base64(file_path, 11)display(IPImage(data=base64.b64decode(base64_image)))\\n\\nWe can then query the model in the usual way. Below we ask it a question on related to the diagram on the page.\\nfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")API Reference:ChatOpenAI\\nfrom langchain_core.messages import HumanMessagequery = \"What is the name of the first step in the pipeline?\"message = HumanMessage(    content=[        {\"type\": \"text\", \"text\": query},        {            \"type\": \"image_url\",            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},        },    ],)response = llm.invoke([message])print(response.content)API Reference:HumanMessage\\nINFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"``````outputThe first step in the pipeline is \"Annotate Layout Dataset.\"\\nOther PDF loaders‚Äã\\nFor a list of available LangChain PDF loaders, please see this table.Edit this pageWas this page helpful?PreviousHow to load Microsoft Office filesNextHow to load web pagesSimple and fast text extractionVector search over PDFsLayout analysis and extraction of text from imagesExtracting tables and other structuresExtracting text from specific sectionsExtracting text from imagesLocal parsingUse of multimodal modelsOther PDF loadersCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwang/workspace/llm_battery_material_supply_chain/.venv3.12/lib/python3.12/site-packages/langchain_community/document_loaders/pdf.py:302: ResourceWarning: unclosed file <_io.BufferedReader name='/tmp/tmp3wgc1_9o/tmp.pdf'>\n",
      "  blob = Blob.from_data(open(self.file_path, \"rb\").read(), path=self.web_path)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2018-03-14T06:25:57+00:00', 'author': 'Jessica Van Brummelen', 'crossmarkdomains[1]': 'elsevier.com', 'crossmarkdomains[2]': 'sciencedirect.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Automotive sensors; Autonomous vehicles; Intelligent vehicles; Localization and mapping; Machine vision; Sensor fusion', 'moddate': '2018-03-14T06:25:57+00:00', 'subject': 'Transportation Research Part C, 89 (2018) 384-406. doi:10.1016/j.trc.2018.02.012', 'title': 'Autonomous vehicle perception_ The technology of today and tomorrow', 'doi': '10.1016/j.trc.2018.02.012', 'robots': 'noindex', 'source': 'https://jessvb.github.io/assets/pdf/Autonomous_Vehicles_Tech_Today_Tomorrow.pdf', 'total_pages': 23, 'page': 0, 'page_label': '384'}\n",
      "\n",
      "Contents lists available atScienceDirect\n",
      "Transportation Research Part C\n",
      "journal homepage: www.elsevier.com/locate/trc\n",
      "Review\n",
      "Autonomous vehicle perception: The technology of today and\n",
      "tomorrow\n",
      "Jessica Van Brummelena, Marie O’Briena, Dominique Gruyerb, Homayoun Najjarana,⁎\n",
      "a Advanced Control and Intelligent Systems Laboratory, University of British Columbia, Kelowna, British Columbia, Canada\n",
      "b Laboratoire sur les Interactions Vehicules, Infrastructure, Conducteurs (LIVIC), IFSTTAR-CoSys-LIVIC, 25 alle des Marronniers, 78000 Versailles,\n",
      "France\n",
      "ARTICLE INFO\n",
      "Keywords:\n",
      "Automotive sensors\n",
      "Autonomous vehicles\n",
      "Intelligent vehicles\n",
      "Localization and mapping\n",
      "Machine vision\n",
      "Sensor fusion\n",
      "ABSTRACT\n",
      "Perception system design is a vital step in the development of an autonomous vehicle (AV). With\n",
      "the vast selection of available oﬀ-the-shelf schemes and seemingly endless options of sensor\n",
      "systems implemented in research and commercial vehicles, it can be diﬃcult to identify the\n",
      "optimal system for one’s AV application. This article presents a comprehensive review of the\n",
      "state-of-the-art AV perception technology available today. It provides up-to-date information\n",
      "about the advantages, disadvantages, limits, and ideal applications of speciﬁc AV sensors; the\n",
      "most prevalent sensors in current research and commercial AVs; autonomous features currently\n",
      "on the market; and localization and mapping methods currently implemented in AV research.\n",
      "This information is useful for newcomers to the AVﬁeld to gain a greater understanding of the\n",
      "current AV solution landscape and to guide experienced researchers towards research areas re-\n",
      "quiring further development. Furthermore, this paper highlights future research areas and draws\n",
      "conclusions about the most eﬀective methods for AV perception and its eﬀect on localization and\n",
      "mapping. Topics discussed in the Perception and Automotive Sensors section focus on the sensors\n",
      "themselves, whereas topics discussed in the Localization and Mapping section focus on how the\n",
      "vehicle perceives where it is on the road, providing context for the use of the automotive sensors.\n",
      "By improving on current state-of-the-art perception systems, AVs will become more robust, re-\n",
      "liable, safe, and accessible, ultimately providing greater eﬃciency, mobility, and safety beneﬁts\n",
      "to the public.\n",
      "1. Introduction\n",
      "Autonomous vehicle (AV) technology is making a prominent appearance in our society in the form of advanced driver assistance\n",
      "systems (ADAS) in both research and commercial vehicles. These technologies aim to reduce the amount and severity of accidents,\n",
      "increase mobility for people with disabilities and the elderly, reduce emissions, and use infrastructure more eﬃciently (Fagnant and\n",
      "Kockelman, 2015). One of the major motivations accelerating the advancement of AV technologies is their insusceptibility to human-\n",
      "related errors, such as distraction, fatigue, and emotional driving, which currently cause approximately 94% of accidents according to\n",
      "a statistical survey completed by the National Highway Traﬃc Safety Administration (NHTSA) (Singh, 2015).\n",
      "As research, testing, and deployment of vehicles with AV technology is escalating around the world, the development of stan-\n",
      "dardized guidelines and regulations has become a major focus to ensure safe integration into society. The U.S. Department of\n",
      "https://doi.org/10.1016/j.trc.2018.02.012\n",
      "Received 14 July 2017; Received in revised form 17 January 2018; Accepted 16 February 2018\n",
      "⁎ Corresponding author.\n",
      "E-mail addresses: jess.vanbrummelen@gmail.com (J. Van Brummelen),marie.lynn.obrien@gmail.com (M. O’Brien), dominique.gruyer@ifsttar.fr (D. Gruyer),\n",
      "homayoun.najjaran@ubc.ca (H. Najjaran).\n",
      "Transportation Research Part C 89 (2018) 384–406\n",
      "Available online 07 March 2018\n",
      "0968-090X/ © 2018 Elsevier Ltd. All rights reserved.\n",
      "T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1134632/782906152.py:18: RuntimeWarning: coroutine 'load_pages' was never awaited\n",
      "  pages = await load_pages()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Load Raw Data ===\n",
    "# Load web resource with Langchain, not including PDF\n",
    "url = \"https://python.langchain.com/docs/how_to/document_loader_pdf/\"\n",
    "loader = WebBaseLoader(url)\n",
    "documents = loader.load()\n",
    "print(documents)\n",
    "\n",
    "# Load PDF with Langchain (works for both local and online PDF)\n",
    "file_path = \"https://jessvb.github.io/assets/pdf/Autonomous_Vehicles_Tech_Today_Tomorrow.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "async def load_pages():\n",
    "    pages = []\n",
    "    async for page in loader.alazy_load():\n",
    "        pages.append(page)\n",
    "    return pages\n",
    "\n",
    "pages = await load_pages()\n",
    "full_text = \"\\n\".join([p.page_content for p in pages])\n",
    "metadata = pages[0].metadata if pages else {}\n",
    "\n",
    "print(f\"{pages[0].metadata}\\n\")\n",
    "print(pages[0].page_content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract sub-links on a homepage of website\n",
    "\n",
    "# from urllib.parse import urljoin\n",
    "\n",
    "# visited = set()\n",
    "# max_depth = 2  # Prevents infinite loops\n",
    "\n",
    "# def crawl_nested_tabs(url, base_url, depth=0):\n",
    "#     if url in visited or depth > max_depth:\n",
    "#         return []\n",
    "\n",
    "#     visited.add(url)\n",
    "\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#     except:\n",
    "#         return []\n",
    "\n",
    "#     text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "#     results = [(url, text)]\n",
    "\n",
    "#     # Find nested links (e.g., subtabs, buttons, inner menus)\n",
    "#     links = [urljoin(base_url, a[\"href\"]) for a in soup.find_all(\"a\", href=True)]\n",
    "\n",
    "#     for link in links:\n",
    "#         if base_url in link:  # Stay within the domain\n",
    "#             results += crawl_nested_tabs(link, base_url, depth + 1)\n",
    "\n",
    "#     return results\n",
    "\n",
    "# homepage = \"https://example.com\"\n",
    "# data = crawl_nested_tabs(homepage, base_url=homepage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Webpage and PDF stored with full metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1134632/1985230621.py:24: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow()\n",
      "/tmp/ipykernel_1134632/1985230621.py:47: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow()\n"
     ]
    }
   ],
   "source": [
    "# Data store in MongoDB\n",
    "# --- Helper functions ---\n",
    "def generate_title_from_url(url: str) -> str:\n",
    "    path = urlparse(url).path\n",
    "    title = path.strip(\"/\").split(\"/\")[-1].replace(\"_\", \" \")\n",
    "    return unquote(title).title()\n",
    "\n",
    "def generate_title_from_filename(filepath: str) -> str:\n",
    "    name = os.path.basename(filepath).replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "    return os.path.splitext(name)[0].title()\n",
    "\n",
    "\n",
    "myclient = MongoClient(\"mongodb://localhost:27017/\")\n",
    "mydb = myclient[\"test_db\"]\n",
    "mycol = mydb[\"test1\"]\n",
    "\n",
    "for doc in documents:\n",
    "    insert_docu = {\n",
    "        \"content\": doc.page_content,\n",
    "        \"metadata\": {\n",
    "            \"source\": url,\n",
    "            \"title\": generate_title_from_url(url),\n",
    "            \"doc_type\": \"webpage\",\n",
    "            \"timestamp\": datetime.utcnow()\n",
    "        }\n",
    "    }\n",
    "    doc_obj = Document(\n",
    "        page_content=insert_docu[\"content\"],\n",
    "        metadata=insert_docu.get(\"metadata\", {})    \n",
    "    ) \n",
    "    query = {\n",
    "        \"page_content\": doc_obj.page_content,\n",
    "        \"metadata.title\": doc_obj.metadata.get(\"title\")\n",
    "    }\n",
    "    if mycol.count_documents(query) == 0:\n",
    "        mycol.insert_one(insert_docu)\n",
    "    else:\n",
    "        print(\"Document already exists.\")\n",
    "\n",
    "single_document = {\n",
    "    \"content\": full_text,\n",
    "    \"metadata\": {\n",
    "        **metadata,\n",
    "        \"doc_type\": \"pdf\",\n",
    "        \"source\": file_path,\n",
    "        \"title\": generate_title_from_filename(file_path),\n",
    "        \"timestamp\": datetime.utcnow()\n",
    "    }\n",
    "}\n",
    "\n",
    "for doc in [single_document]:\n",
    "    doc_obj = Document(\n",
    "        page_content=doc[\"content\"],\n",
    "        metadata=doc.get(\"metadata\", {})    \n",
    "    )\n",
    "    query = {\n",
    "        \"page_content\": doc_obj.page_content,\n",
    "        \"metadata.title\": doc_obj.metadata.get(\"title\")\n",
    "    }\n",
    "    if mycol.count_documents(query) == 0:\n",
    "        mycol.insert_one(doc)\n",
    "    else:\n",
    "        print(\"Document already exists.\")\n",
    "\n",
    "print(\"✅ Webpage and PDF stored with full metadata.\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data retrieve from MongoDB based on timestamp and convert to Langchain format, splitting and embedding\n",
    "def data_retrieval_by_time(client):\n",
    "    db_name = client['test_db']\n",
    "    col_name = db_name['test1']\n",
    "    start_time = datetime(2025, 1, 1)\n",
    "    end_time = datetime(2025, 5, 1)\n",
    "    data = col_name.find({\"metadata.timestamp\": {\n",
    "            \"$gte\": start_time,\n",
    "            \"$lte\": end_time\n",
    "        }   \n",
    "    })\n",
    "    return data\n",
    "\n",
    "results = data_retrieval_by_time(myclient)\n",
    "\n",
    "# Convert to Langchain format\n",
    "langchain_documents = [\n",
    "    Document(\n",
    "        page_content=doc[\"content\"],\n",
    "        metadata=doc.get(\"metadata\", {})\n",
    "    ) for doc in results\n",
    "]\n",
    "\n",
    "# Splitting\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "split_docs = splitter.split_documents(langchain_documents)\n",
    "# print(f\"Total chunks created: {len(split_docs)}\")\n",
    "# print(\"Example chunk:\\n\", split_docs[0].page_content)\n",
    "# print(\"Metadata:\", split_docs[0].metadata)\n",
    "\n",
    "# Embedding\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "texts = [doc.page_content for doc in split_docs]\n",
    "text_embeddings = hf.embed_documents(texts)\n",
    "\n",
    "print(f\"Vector length: {len(text_embeddings)}\")  # Should match model dimension, e.g., 433\n",
    "print(f\"First 5 values: {text_embeddings[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete entire class from Weaviate\n",
    "# client = weaviate.connect_to_local()  # ✅ Version 4.x\n",
    "# client.collections.delete(\"Supply_chain_material\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1134632/3836467440.py:3: ResourceWarning: unclosed <socket.socket fd=93, family=2, type=1, proto=6, laddr=('127.0.0.1', 45946), raddr=('127.0.0.1', 8080)>\n",
      "  client = weaviate.connect_to_local()  # ✅ Version 4.x\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import weaviate.classes as wvc\n",
    "\n",
    "client = weaviate.connect_to_local()  # ✅ Version 4.x\n",
    "# Define collection name\n",
    "collection_name = \"Supply_chain_material\" # Collection name must have the first letter capitalized in Weaviate\n",
    "\n",
    "# Check if collection/class already exists\n",
    "existing_collections = client.collections.list_all()\n",
    "\n",
    "if collection_name not in existing_collections:\n",
    "    questions = client.collections.create(\n",
    "        name=collection_name,\n",
    "        properties=[\n",
    "            wvc.config.Property(\n",
    "                name=\"content\",\n",
    "                data_type=wvc.config.DataType.TEXT,\n",
    "            ),\n",
    "            wvc.config.Property(\n",
    "                name=\"source\",\n",
    "                data_type=wvc.config.DataType.TEXT,\n",
    "            ),\n",
    "            wvc.config.Property(\n",
    "                name=\"timestamp\",\n",
    "                data_type=wvc.config.DataType.DATE,\n",
    "            )\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store/retrieve data to/from Weaviate\n",
    "from uuid import uuid4\n",
    "\n",
    "# Get the collection object\n",
    "collection = client.collections.get(\"supply_chain_material\")\n",
    "\n",
    "# Store documents and vectors\n",
    "for doc, vector in zip(split_docs, text_embeddings):\n",
    "    data_obj = {\n",
    "        \"content\": doc.page_content\n",
    "    }\n",
    "\n",
    "    # Insert the document with vector\n",
    "    collection.data.insert(properties=data_obj, vector=vector, uuid=uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 471 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3282.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   21003.77 ms /   187 runs   (  112.32 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   21102.14 ms /   188 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prominent research and commercial vehicles such as Audi’s Research Vehicle, AutoNOMOS’ s MadeInGermany, Carnegie Mellon’s Urban Challenge entry, “Boss”, Ford’s Hybrid Fusion research vehicle, and Volkswagen Passat use a variety of sensors. These include:\n",
      "LIDAR (Light Detection and Ranging) for surrounding perception, radar (Radio Detection and Ranging) for long-range detection, sonar for detecting obstacles in close proximity, cameras (Vision Stereovision Infrared) for visual perception, and other sensors such as GPS and accelerometers.\n",
      "All of these sensors work together to provide a comprehensive view of the vehicle’s surroundings, allowing it to make informed decisions about how to navigate its environment.\n"
     ]
    }
   ],
   "source": [
    "# Get the collection object\n",
    "collection = client.collections.get(collection_name)\n",
    "\n",
    "query_question = 'Which sensors are currently used in prominent research and commercial vehicles?'\n",
    "query_vector = hf.embed_query(query_question)\n",
    "results = collection.query.near_vector(query_vector, limit=3)\n",
    "retrieved_chunks = [obj.properties[\"content\"] for obj in results.objects]\n",
    "\n",
    "# LLM prompt\n",
    "\n",
    "context = \"\\n\".join(retrieved_chunks)\n",
    "prompt = f\"\"\"Answer the question using the following context:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query_question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "response = llm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune pre-trained LLM--LLaMA2 with LoRA algorithm\n",
    "\n",
    "# Step 1: Connect to Weaviate and fetch documents\n",
    "\n",
    "response = client.query.get(collection_name, [\"content\"]).with_limit(1000).do()\n",
    "documents = [doc[\"content\"] for doc in response[\"data\"][\"Get\"][collection_name]]\n",
    "\n",
    "# Step 2: Prepare HuggingFace dataset\n",
    "dataset = Dataset.from_dict({\"text\": documents})\n",
    "\n",
    "# Step 3: Load pre-trained tokenizer and model\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # Or another quantized variant\n",
    "token = \"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\" # huggingface token\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\",  # Automatically use GPU if available\n",
    "                                                torch_dtype=torch.bfloat16, token=token,load_in_8bit=True) \n",
    "\n",
    "# Step 4: Apply LoRA for lightweight fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Step 5: Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Step 6: Set up Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama2-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "# Step 7: Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 8: Save model\n",
    "model.save_pretrained(\"llama2-finetuned\")\n",
    "tokenizer.save_pretrained(\"llama2-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are currently 195 recognized sovereign states in the world. This number includes 193 Member States of the United Nations, 2 Observer States (the Holy See and Palestine), and 2 territories (Taiwan and Western Sahara) that are not fully recognized as sovereign states but have been granted observer status in international organizations.\n",
      "The list of sovereign states in the world is as follows:\n",
      "1. Afghanistan\n",
      "2. Albania\n",
      "3. Algeria\n",
      "4. Andorra\n",
      "5. Angola\n",
      "6. Antigua and Barbuda\n",
      "7. Argentina\n",
      "8. Armenia\n",
      "9. Australia\n",
      "10. Austria\n",
      "11. Azerbaijan\n",
      "12. Bahamas\n",
      "13. Bahrain\n",
      "14. Bangladesh\n",
      "15. Barbados\n",
      "16. Belarus\n",
      "17. Belgium\n",
      "18. Belize\n",
      "19. Benin\n",
      "20. Bhutan\n",
      "21. Bolivia\n",
      "22. Bosnia and Herzegovina\n",
      "23. Botswana\n",
      "24. Brazil\n",
      "25. Brunei\n",
      "26. Bulgaria\n",
      "27. Burkina Faso\n",
      "28. Burundi\n",
      "29. Cambodia\n",
      "30. Cameroon\n",
      "31. Canada\n",
      "32. Central African Republic\n",
      "33. Chad\n",
      "34. Chile\n",
      "35. China\n",
      "36. Colombia\n",
      "37. Comoros\n",
      "38. Congo (Brazzaville)\n",
      "39. Costa Rica\n",
      "40. Côte d'Ivoire\n",
      "41. Croatia\n",
      "42. Cuba\n",
      "43. Cyprus\n",
      "44. Czech Republic\n",
      "45. Denmark\n",
      "46. Djibouti\n",
      "47. Dominica\n",
      "48. Dominican Republic\n",
      "49. Ecuador\n",
      "50. Egypt\n",
      "51. El Salvador\n",
      "52. Equatorial Guinea\n",
      "53. Eritrea\n",
      "54. Estonia\n",
      "55. Ethiopia\n",
      "56. Fiji\n",
      "57. Finland\n",
      "58. France\n",
      "59. Gabon\n",
      "60. Gambia\n",
      "61. Georgia\n",
      "62. Germany\n",
      "63. Ghana\n",
      "64. Greece\n",
      "65. Grenada\n",
      "66. Guatemala\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import google.generativeai as genai\n",
    "\n",
    "\n",
    "# # Configure the API\n",
    "# genai.configure(api_key=os.environ[\"Google_API_KEY\"])\n",
    "\n",
    "# # Load the Gemini model\n",
    "# model = genai.GenerativeModel(\"models/gemini-1.5-flash\")\n",
    "\n",
    "# # Ask a question\n",
    "# question = \"find BYD battery material production sites, list top 10\"\n",
    "# response = model.generate_content(question)\n",
    "\n",
    "# # Print the response\n",
    "# print(\"Answer:\")\n",
    "# print(response.text)\n",
    "\n",
    "# from langchain.llms import LlamaCpp\n",
    "\n",
    "# llm = LlamaCpp(\n",
    "#     model_path=\"../models/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "#     n_ctx=2048,\n",
    "#     temperature=0.7,\n",
    "#     max_tokens=256,\n",
    "#     verbose=False\n",
    "# )\n",
    "\n",
    "# response = llm(\"how many countries in the world?\")\n",
    "# print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
